{"filename": "Candidate2-AWS_Data", "pages": [{"page_n": "1", "p_content": " Masood Ahmad  M: 07429 049299 | Email: masoodahmad@smthree.co.uk | LinkedIn: www.linkedin.com/in/masood-ahmad-877a8a17/   Executive Summary  Enterprise level architect, accomplished and award-winning Data Modeller, Data Architect and Data Analyst with a progressive career spanning over 20 years. Delivered complex and business-critical data projects in application and technology domains for financial, media, public, insurance, consulting and education sector clients.  Specialises in big data, cloud, business intelligence and analytics architecture.  Offers an array of skills in data platforms, machine learning, enterprise architecture, data governance, data security, data privacy, data engineering, data-warehouse, streaming data, open-source, project management, team management and budget management. Experienced in understanding business strategy, requirements and processes and defining, designing and implementing effective data solutions. TOGAF, Oracle and Hadoop certified. Amazon AWS and Google GCP architect with a consistent track record of delivering projects through conceptual, logical and physical stages using structured change framework and team collaboration.   Areas of Expertise: \u00a7 Data Modelling, Data Architecture, Data Analysis  \u00a7 Metadata, Reference Data, Master Data \u00a7 Data Security, Data Privacy, GDPR Regulations \u00a7 Conceptual, Logical, Physical data design  \u00a7 RDBMS, NoSQL, Graph, Document, Key-Value Database  \u00a7 Data Warehouse, Data Lake, Hive, Hadoop, Big Data \u00a7 REST, Event Driven, Streaming, Micro-services \u00a7 Oracle, Postgres, Redshift, SQL Server, Big Query  \u00a7 SQL, PL/SQL, Python, Spark, Neo4j Cypher \u00a7 Scrum and Kanban SDLC, Amazon AWS, Google GCP \u00a7 Budget Management, Vendor Management \u00a7 Team Leadership, Training, Coaching and Mentoring \u00a7 Stakeholder Management, Client Engagement   Notable Career Highlights  v As part of Enterprise Architecture team, designed HLA (High Level Architecture) for cloud based data platform for integrating insurance (Quote, Policy, Billing, Claim) data from Guidewire, Sales Force and external resources  using Mule Soft, Kafka Streaming, Informatica ETL, AWS services (S3, Lambda, Redis-Cache Dynamodb, Athena, Redshift) for operational, MI and data analytics purpose, took design approvals through Technical Authority  and Design Authority (CTO Level), introduced data governance using data models, data dictionaries, master data and reference data v Architected data platform migration from on-premises to cloud (AWS) for Flexera including data-modelling, logical and physical designs from relational to graph (Neo4j), vendor management. Additionally built organisation capability in ETL pipeline, AWS cloud, NoSQL (Dynamodb), data-warehouse (Redshift), graph Cypher and S3 based Data-Lake  v Successfully directed and coordinated the delivery of two major data projects for Deutsche Bank ensuring alignment with stringent deliverables \u2013 migration of US-FED5G and US-LCR Reporting from SAP-HANA to Oracle Enterprise Data Warehouse including Kimball dimensional data-modelling, ETL enhancements and built Big Data Analytics platform on Hadoop and Spark which resulted in impressive cost savings of \u00a33m quarterly  v Conceptualised data standardisation and enrichment for HSBC Fixed Income Reengineering programme using Microgen Aptitude Hub and PL/SQL for trades from Martini, Bloomberg, Murex, Sophis and Calypso.  Additionally extended application data-model for staging, hopper, reference and lookup table structures for general ledger posting and reconciliation, and ETL automation for front-to-back straight-through-processing (STP) v Led and coordinated VTB Front office Calypso application data improvement programme, performed gap analysis, prepared plan and delivery road map for business-critical service to manage data growth with technical features in Oracle and surrounding change management, testing, UAT and release management processes v Played a pivotal role in extending data warehouse model using Kimball dimensional design for Barclays Flow and Proprietary FX trading risk calculations, intra-day and end-of-day liquidity reporting and serving business requirements,  used Oracle partitioning and compression feature for stability of 300m/day FX transactions v Spearheaded the end to end delivery of IB-Prime Brokerage application migration from Sybase to Oracle technology, including data-modelling, improved data quality for testing and development, collaborated with global architect team for standardizing data environments, trained and mentored application team for Oracle features, fulfilled data requirements for QA, SIT and UAT stringent monthly release deadlines.  v Recognised for meeting business operational stability goal at JP Morgan-Treasury which involved root cause analysis and implementation of performance solution for Oracle business critical applications including Global FX (CLS Bank), iPAY (Payments), SWIFT (Financial Messaging) and IDDA (demand Deposits), resulted in achievement of the \u2018Spot Award\u2019 v Received several \u2018Ovation Awards\u2019 from GE-NBC Universal for successfully leading EMEA data team, delivering business goals for SAP-FICO and SAP-BW operational excellence, integrating business applications after acquisitions, budget management, vendor management, data centre consolidation and building motivated and performant EMEA team  \f"}, {"page_n": "2", "p_content": " Career History   Data MODELLER-DATA ARCHITECT: Deloitte                                                                   February 2020 \u2013 To Present  Data Modeller and Data Architect for Network Rail-Digital Rail Programme \u00a7 Data Modeller for defining and designing consolidated conceptual, logical and physical data model for Digital Rail programme including network rail assets, operations and train time-tables using ERWIN \u00a7 Data Architect for designing Google cloud based architecture for integrating various data sources into data lake and Big Query data warehouse used by machine learning models for Network Rail train timetable analytics and discovering operational efficiency from improving PPM (Public Performance Measure)  PRINCIPAL CONSULTANT-DATA EVANGELIST: Kubrick Group                          November 2019 \u2013 February 2020  Lead and Train junior data consultants and data scientists for AWS cloud, data modelling and data warehousing  Data architect, consultant and advisor for corporate clients data programs  \u00a7 Lead, train and mentor junior data engineers, data scientists and data analysts for AWS cloud data platform i.e. RDS, S3 Data Lake, Redshift, relational data design, E-R data modelling using ERWIN, Kimball data warehouse, SQL, DAMA DMBOK including data governance and big data processing patterns \u00a7 Engage with corporate clients across industries including banking, hedge funds, real estate, retail, car manufacturer, telecommunication, travel, media, insurance and airline for data and cloud transformation solution architecture \u00a7 Manage internal innovation lab for defining, designing and implementing client specific use case prototype solutions including end-to-end architecture for AWS cloud services, Alteryx for data analysis, Collibra for data governance and Tableau for data visualization  ENTERPRISE DATA ARCHITECT: Capgemini-Direct Line Insurance Group   May 2019 \u2013 October 2019  High Level Architecture for integrating Guidewire, Sales Force, Digital and Web channels on AWS cloud using Mule Soft: \u00a7 Member of Enterprise Architecture team responsible for End-to-End design and integration of B4C (Best for Customer) programme organized for on premises to cloud migration and for establishing new business services in cloud \u00a7 Engaged with business and technical teams for understanding requirements and use cases, designed appropriate relevant solutions for achieving business objectives in cost effective manner \u00a7 Designed End-to-End (e2e) High Level Architecture (HLA) for standardising customer marketing preferences, across all brands, using REST APIs among Sales Force CRM, Digital (Web) channels, Mule Soft integration layer and AWS platform \u00a7 Designed End-to-End (e2e) High Level Architecture (HLA) for ingesting, storing and utilizing customer car Telematics/IoT for policy/driver score and journey details using REST APIs among Floow (Telematics), Mule Soft, Kafka Streaming, Java based en-queue/ de-queue, S3 data-lake, Informatica ETL and Redshift OLAP \u00a7 Identified, proposed and established PostgreSQL as an alternative RDBMS solution which is low cost, open source and ANSI SQL compliant system, resulted in reducing Oracle license expense by nearly \u00a31 million \u00a7 Contributed and advised data models for relational (Oracle), NoSQL (Dynamodb) and data-warehouse (Redshift) systems \u00a7 Prepared Visio designs, Archimate artefacts, Power-Point presentations for architecture repository and engaging with technical and business users including senior stakeholders, programme, analyst and delivery teams \u00a7 Designed data governance framework using data models, data dictionaries, data catalogue using Collibra data tool  \u00a7 Identified and utilized various AWS cloud services including S3 data-lake, Lambda, Redis-Cache, Dynamodb, Redshift, EC2, VPC, Kinesis, RDS, EMR and Spectrum for optimizing enterprise data-platform  \u00a7 Contributed to data integration design between Sales Force and Guidewire Insurance Suite  \u00a7 Presented and achieved solution approvals from Technical Design Authority (TDA) and Design Authority (CTO Level)   DATA ARCHITECT: Flexera         May 2018 \u2013 May 2019  Designed and built data platform on AWS cloud and migrated on-premises relational-Oracle to graph-Neo4j model: \u00a7 Appointed to design, develop and implement a AWS cloud-based data platform within the Enterprise Centre of Excellence ensuring alignment with stringent timelines, quality standards and business requirements \u00a7 Provided leadership and direction to a team of 3 Data Engineers to build a cloud data platform whilst being a key member of the global enterprise architect team with ownership for designing business cloud vision and strategy \u00a7 Designed and implemented graph based data model including logical and physical designs and converted data from relational (Oracle) to Neo4j graph connected shape in collaboration with Business Analysts team \u00a7 Managed and coordinated the end to end data migration from on-premises to cloud (AWS) using a complex ETL pipeline in Python, Spark, PostgreSQL, SQL, graph Cypher and Neo4j \u00a7 Architected solutions for AWS NoSQL Dynamodb table, Redshift data-warehouse and AWS S3 based data-lake \u00a7 Worked in Agile (Scrum) SDLC using sprints, Git, Jira, Confluence and SharePoint collaboration, oversaw and mentored the application transition from SQL to Cypher language and negotiated with vendors for software license costs and support of Neo4j technology solution \u00a7 Utilized AWS based services including RDS, EC2, S3, ECS, Kinesis Streams, Serverless, Lambda, Docker and Jenkins for CI/CD Blue/Green deployments, Logz.io (Elasticsearch), Prometheus, Grafana and DC/OS   \f"}, {"page_n": "3", "p_content": " DATA ARCHITECT: [GFT] Deutsche Bank       Feb 2016 \u2013 Dec 2017 \u00a7 Directed and coordinated the end to end delivery of data solution for two major projects at bank Treasury IT:  Migration of US-FED5G and US-LCR reports from SAP-HANA to Oracle Data Warehouse and Business Objects: \u00a7 Managed the migration of USA regulatory reports from SAP HANA to Oracle data-warehouse, understood business requirements, created data mapping, metadata, Kimball dimensional data modelling and ETL straight-through-processing  \u00a7 Accountable for managing and coordinating across Treasury data hub team to facilitate successful production of daily regulatory LCR and FED5G reports from Business Objects \u00a7 Designed, developed and tested in agile (Scrum) SDLC using Git and Jira, worked with Business Analysts for refining requirements, defining EPICs, stories, tasks, backlog grooming, prioritization, work estimation and technical solutions \u00a7 Integrated FED rules into the Treasury Data Hub design to generate daily XSD validated reports in XML format, and performance tuned feeds and reports in SQL and PL/SQL for intra-day liquidity programme   Big Data Analytics Platform Build Project including Hadoop, Spark, Graph and Tableau: \u00a7 Utilised Spark, Hadoop,, ElasticSearch, graph/Neo4j and Tableau to design and build the new Big Data Analytics platform with ownership for designing data acquisition, data transport, data storage and data transformation solution \u00a7 Designed and developed logical and physical  data models, managed the integration of structured and unstructured data from upstream feeds into Hadoop using Spark based ETL \u00a7 Conceptualised and designed NoSQL flattened data model solution for merging relational positions and cashflow fact tables into single 300+ columns table in Hadoop and Hive for data consolidation and source for multiple reports \u00a7 Carried out cost benefit analysis of Cloud implementation options including IAAS, PAAS and SAAS and investigated a range of database options including NoSQL, Graph and Data-Lake  DATA ARCHITECT: [Grant Thornton] BNP Paribas        Nov 2015 \u2013 Feb 2016 Finance, Accounting and Reporting for Liquidity Cover Ratio, Balance Sheet and P&L: \u00a7 Key member of the Corporate and Institutional Banking Regulatory Reporting Data Analysts team covering the UK, USA and France, encompassing LCR reports for Fixed Income, Global Equities, Commodities and Treasury \u00a7 Conceptualised, developed and documented Oracle PL/SQL ETL processes using diverse data sources and worked closely with Business Analysts to identify functional requirements and prepare technical solutions for regulatory reporting \u00a7 Produced data models for new business requirements and played a pivotal role in reviewing the physical design of the Oracle 12c database in respect of data lifecycle management, partitioning, performance and best practices \u00a7 Built, managed and maintained trusted and collaborative relationships with the USA business analysts team to understand, manage and produce US-LCR regulatory reporting requirements  \u00a7 Instilled Agile Scrum working practices and utilised Jira for workload management, estimates and release management   DATA ARCHITECT: [Resource Solutions] HSBC Investment Bank     Feb 2014 \u2013 Sep 2015 Fixed Income Architecture (FIAR) Programme using Microgen Accounting Hub: \u00a7 Responsible for Microgen Accounting Hub System to adjust new data feeds and transformations into general ledger, and designing new functionality for global instrument reference data management and for developing the ETL framework \u00a7 Tasked with leading the Oracle 11g PL/SQL development for a complex Fixed Income Architecture Re-engineering (FIAR) programme encompassing bonds, repos, valuations, settlements, positions and cash flows using front-to-back STP \u00a7 Conceptualised, developed and implemented an ETL solution using PL/SQL for data quality, standardisation, enrichment and loading into sub-ledgers for down-stream feeds to the group general ledger \u00a7 Worked in close collaboration with Business Analysts with ownership for accounting rules implementation, general lookups, data staging, migration and standardisation processes using Agile Scrum methodologies \u00a7 Oversaw and ensured best practices were consistently adhered to during unit testing, peer code review, system testing, SIT, UAT and IBM Rational Team Concert for source control, environment builds and release management  ASSOCIATE DIRECTOR: VTB Capital        Aug 2013 \u2013 Feb 2014 Front Office Calypso Application Improvement Programme: \u00a7 Led the review, evaluation and improvement of the Calypso application in data domain, including business data retention policy to ensure alignment with regulatory obligations, and designed robust data performance solutions \u00a7 Managed the successful deployment of the Oracle E-Business application on Oracle 11g for HR module and additionally led the implementation of OBIEE for business critical reporting and data ETL from Oracle E-Business applications \u00a7 Performed Oracle 11g database gap analysis, managed standards across all business applications and directed the end to end data migration and upgrade to Oracle 11g RAC on RedHat Linux   DATA WAREHOUSE ARCHITECT: Barclays Capital      Jul 2012 \u2013 Aug 2013 FX Flow and Proprietary Trading Data-warehouse Programme: \u00a7 Provided strategic and tactical guidance and recommendations regarding data warehousing best practices \u00a7 Designed data warehouse Kimball dimensional model for refining and enhancing business reporting and played a key role in the data warehouse standards committee with the aim of identifying and implementing latest Oracle features  \u00a7 Spearheaded the successful implementation of Oracle 11g table sub partitioning  and data compression solutions for P&L \f"}, {"page_n": "4", "p_content": " intra-day and end-of-day risk reporting performance improvement  \u00a7 Prepared best practices for storing and retrieving XML based data in Oracle 11g utilizing PL/SQL, XML and XQuery-XPath technology features to build and implement ETL processes \u00a7 Managed internal and external resources, timelines and milestones across the business and provided leadership and mentoring to the RTB team regarding Oracle best practices for diagnostics and monitoring  DATA ARCHITECT: J.P. Morgan        Apr 2010 \u2013 Jul 2012 Treasury Application Improvement Programme in Bournemouth: \u00a7 Led and implemented iPAY(Payments), FX Settlements(CLS Bank), Financial Messaging (SWIFT), iDDA(Demand Deposits) Oracle architecture, database stability, SQL performance and ODS improvements initiatives, won \u201cSport Award\u201d IB Prime Brokerage Application Improvement Programme in London: \u00a7 Established and mentored Oracle capability for application team after migrating from Sybase, team training, reference and market data enhancements, data quality, data modelling, PL/SQL rollout, OLTP and data-warehouse segregation for business reporting, coordination and collaboration with global architect team for data standards and governance   MANAGER | DATA ARCHITECT: GE-NBC Universal      Feb 2007 \u2013 Apr 2010 \u00a7 Managed EMEA on-site and out-sourced (PATNI) offshore (India) data teams with plan, build and implement responsibility for all data projects across the business including ERP systems (SAP-FICO, SAP-BW), 3rd party business applications, vendor management, budget management, global (New York, Los Angeles and UK) team collaboration \u00a7 Played pivotal role in integrating business applications after various business acquisitions, GE data centre consolidation, motivating and enhancing team delivery capacity, data security, SOX compliance, won multiple \u201cOvation Award\u201d for various contributions to business success   Early Career Summary  DATA WAREHOUSE ARCHITECT (Department for Works and Pensions, Labour Market System): DWP Dec 2004 \u2013 Feb 2007 ORACLE TRAINER (Instructor Led Classroom trainer for Oracle Software): Oracle University         Nov 2002 \u2013 Sep 2004  Academic and Professional Certifications   \u00a7 MBA in Finance (CGPA 3.2): 1996 \u00a7 BA in Advanced Mathematics: 1991 \u00a7 TOGAF 9.2 Certified \u00a7 Cloudera Certified Developer for Apache Hadoop v5 \u00a7 Oracle 8, 8i, 9i, 10g, 11g Certified Professional (OCP)  Professional Training  \u00a7 Various online courses for Amazon AWS Cloud and Google GCP Cloud  \u00a7 Machine Learning \u00a7 GDPR \u2013 Data Privacy  Technical Skills  Amazon AWS Cloud | Google GCP Cloud | Data Modelling | Data Architect | NoSQL | SQL | PL/SQL | Data-Warehouse | OLTP | Python | Kimball | STAR Schema | Toad | Erwin | SQL-Modeller | SDLC | Visio | Excel | PowerPoint| Shell Scripting | Agile | HDFS | Hadoop | Hive | Sqoop | Flume | Spark | Linux | Unix | Oracle 10g, 11g, 12c | Performance Tuning | RAC | XML | XSD | Tableau | Business Objects | Microgen Aptitude Hub | Team City | Control-M | SVN | GIT | GIT Stash | SQL*Loader | PYSPARK | JUPYTER Notebook | Pandas | NumPY | Spark SQL | Machine Learning | Spark Data-Frame | Spark RDD | SCRUM| KANBAN | Graph Data Model | Kinesis | Cypher | Dynamodb| ElasticSearch | Micro-Services | JSON | Neo4J | Data Lake | S3 | Lambda | Serverless | EC2 | Docker | Jenkins | ECS | Data Vault | Logical & Physical Data design | GDPR | Data Privacy | MongoDB | Enterprise Architect | Guidewire | Kafka | Redshift | Mule Soft | Insurance | Investment Banking | TOGAF | Event Driven Architecture | SOA | Micro Services | Streaming Architecture | Big Query | REST \f"}]}