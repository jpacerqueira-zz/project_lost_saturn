{"filename": "cv-x1", "pages": [{"page_n": "1", "p_content": "Big Data & Data Science Architect   PERSONAL INFORMATION  Name:\u200b Jo\u00e3o P. A. Cerqueira  Sex:\u200b \u200bMale\u200b  Date of birth: \u200b19/12/1983  Address UK: \u200b21, Woodlands Avenue, Worcester Park, KT47AL Contacts:\u200b (+44) 07572550311 Email:\u200b \u200bjpacerqueira@gmail.com Skipe:\u200b jpacerqueira83 Linkedin: \u200buk.linkedin.com/in/joaopedroafonsocerqueira github: \u200bgithub.com/jpacerqueira personal site : \u200bhttps://fuelbigdata.com   WORK EXPERIENCE   \u25cfLead Data Consultant - Data Science and Data Engineering - GFT Financial Limited (From June2019 - current Date)  GFT Financial Limited  - City of London (\u200b \u200bhttps://www.gft.com/uk/en/index/\u200b ) (Permanent)  Business or sector: \u200bConsultancy services via GFT Financial Limited UK, for non-financial services customers.  Technologies used: 1.Design and implement an Multi-Cloud solution for Data Science Analytics, fous in Python solutions. 2.Setup of a Docker Hub solution for Data Science using : 1.Python 2.7 - 3.6 2.H2o.ai free tier 3.Spark latest version up to 2.4.x  4.Establish of Prototypes as consumable Docker Hub images, valid in Multi-cloud and private Cloud.  5.Establish a public contributor repo for democratic usage of the solution. 1.Detials in \u200bhttps://github.com/jpacerqueira/Jupyter_Spark_H2O_Kafka_Client_Setup 3.Establishment of my consultancy in GFT using the principles of Fuel BigData . 1. An entity dedicated in Data Science consultancy. 2.Applied Full Stack Data Science with Business Intelligence, Analytics and Machine Learning tools, principles and solution definitions. 3.Follow site for more details  \u200bhttps://fuelbigdata.com\u200b .  Project roles :  1.Got involved in new customer engagement, supporting new sales pitch for new markets. Establish principles and define general technical solution approach for Data Science Multi-cloud (GCP, Azure, AWS) 2.Lead Data Architect for the 1st UK customer outside Financial Services, the customer is \f"}, {"page_n": "2", "p_content": "\u201cundescribed in retail Property Sector\u201d . Establish the relationship with the customer with their new Technology department. Bring a foundation solution for their requirements around Business Intelligence and Data Collection at the business level. Implement the architecture solution in Azure Cloud . Define A new Solution for data consumption and servicing using Azure Citus PostGreSQL to collect, exchange and service information to partners, via a WebApp solution. Define a General Approach for Analytics for such application, with ETL support using Python and PySpark with Apache Arrow accelerators. Solution at TPOC level, was handled for development with GFT nearshores.   \u25cfBig Data & Data Science  Architect - Head of Big Data - Perform Media Group (From November 2015 \u2013 June2019 )  DAZN Media Group ( ex- Perform Media)  - London (\u200bhttp://www.performgroup.com/contact/\u200b) (Permanent)  Business or sector: \u200bPerform Group is a Sports Media group owner of Brands: Opta, Runningball, Goal.com, SportingNew.com and DAZN.com ( OTT b2c video subscription).  Technologies used: 4.Design and implement the Big Data full stack at Perform Group using the following technologies: 1.Cloudera Enterprise Data Hub ( PaaS ) , CDH 5.9 and recently CDH 5.14 . 2.Oracle enterprise Big Data Appliance ( BDA) , with Cloudera CDH. 3.Implemented the new generation of IOT collection from tracking data supply of tracking cameras to enrich Opta event data. Usage of Akka collection tools using Fast Data principles and Kafka buffer storage for Spark Streaming analytics. 4.Implemented a \u201cSingle Customer View\u201d across the Portals of Perform Group , goal.com sportingnew.com , dazn.com , dabblebet.com and others to exchange customer profiles and unique_id between DW, Marketing Cloud CRM and individual site profiles. Designed a solution using Neo4J for graph calculation and Big Data Cloudera CDH for log analysis. 5.Design of a new AWS Cloud solution for DW for DAZN product using AWS EMR, RedShift, RedShift Spectrum, AWS Glue Crawlers. 1.Design the new generations of S3 lake based on Object Store. 2.Design the new generation of MetaStore collection using Schema on read over object store with AWS Glue Crawlers. 3.Enable the reception of Media Portal profiles and customers in the new S3 Lake, from the on-premise solution designed as Single Customer View for Perform Media Services. 6.Designing a New general framework for ETL Solutions based on: 1.Control of /raw /staged /published areas 2.Utilize ETL technologies Apache Spark 1.6.0/2.1.0/2.3.0 ( Python and Scala) 3.Achieve published insights for apache Hive. 4.Served insights , on-premise using Oracle Big Data SQL. 5.Served insights, in-cloud AWS with using RedShift Spectrum. 7.Full setup of Data Science Stack for Brands Opta and Perform Media 1.Used technologies based on Python and R languages for data science. 1.R-Studio and R as a data science stack of the new generation of Perform logic. 2.Setup of services with Oracle R Enterprise (ORE dbi) for ORACLE DB. 3.Setup for  BigData with Hadoop/Hive using ORCH R/cran.  1.Design of processes and setup of services for the Data Science team, to align in the SDLC of development teams.  2.Setup of data science services with sparklyr and tools of a new generation of machine \f"}, {"page_n": "3", "p_content": "learning and Deep learning services using R H20.ai .  4.Setup for  BigData with Hadoop/Hive using PySpark. 1.Design of processes and setup of services for the Data Science team, to align in the SDLC of development teams.  2.Setup of DataScience stack with pyspark, sparkling water, h2o.ai services and tools for a new generation of machine learning and deep learning services using Python H20.ai.  8.Opta services, setup of SOLR search from collected Sports Data, and sports fixtures ( uuids for sports venues, players, clubs, leagues, tournaments, seasons) . 9.Setup of Oracle Big Data Discovery visual tool over on-premissine BDA cluster. 1.Usage of lake with published Apache Hive in Spark context jobs.  10.Setup of H2o Deeplearning and Boosting solutions, using R.  1.Implementation of decision pipelines for content distribution for DAZN.  1.A solution T-POC for Streaming AVGBitRate evaluation of normal/abnormal conditions, in http://bit.ly/2nFHYpf\u200b \u200b. 2.A solution T-POC for Malware/Phishing intrusion detection and prediction. Designed with url based evaluation , based on external/internal data and conditions, in \u200bhttp://bit.ly/2qAISok\u200b .  11.Establishment of Fuel BigData . 1. An entity dedicated for Ai consultancy. 2.Follow site for more details  \u200bhttps://fuelbigdata.com\u200b . Project roles :  3.Lead Architect for the new Sports Media solution with third party tracking data , being matched with internal insights of sports event data. Production of new generation of Sports insights, from an event, to an even here, where, with this intervenients around the events. Control of the Software Development and Data science function support of new development algorithms and tools. Represented Perform Group at the Oracle OpenWorld 2017 in Intel Keynote sessions where solution was presented with or Partner Oracle ( \u200byoutube link\u200b ) . Presented the project in a variety of Oracle and Cloudera events ( \u200bcloudera link\u200b ). 4.Lead Architect for BigData solution for the new generation of Sports Fans insights across social media, designed and implementation of the social media collection tools, using Data streams from Omniture and from Google Analytics collection Google DFP for Adds Data, Gygia social registrations for Facebook, Twitter and G+. Implementation of Google Analytics, AdWord solution and Google Ads( ex- DPF) for market understanding and segmentation. Implementation of a new generation of Marketing Cloud DMP, for Goal.com and DAZN.com brands using Bluekai from Oracle Cloud. Full deployment and end to end with reporting using Oracle big Data , and Bluekai Cloud solutions. 5.Lead Architect for a Sport integrity for Betting solutions. Used Oracle Big Data SQL ( 3.1) for data servicing of a new generation of Spark/ML and SparkLyR context analysis over sports Data. Publication of  data from Hive database , with transformations over collected data in SOLR search, for abnormal pattern detection. 6.Lead Architect for the new DAZN EDW solutions, coordinating the teams to scale a new platform for AWS Cloud with Big Data using solutions, AWS Glue, AWS RedShift/Spectrum, AWS EMR, AWs Kinesis.  \u25cfSenior Software Developer - EDM Big Data  (From January 2015 \u2013 October 2015 )  SKY , CBS - EDM Osterley (\u200bhttps://corporate.sky.com/contact-us\u200b \u200b) (Permanent)  Business or sector: \u200bSystems Migration from Cloudera CDH 4.3 to Cloudera CDH 5.2 with ETL and CRM techniques for Media and Core product content Analytics.  Technologies used: \f"}, {"page_n": "4", "p_content": "12.Designing ETL Solutions with Apache Spark 1.3 and production of insights for ElasticSearch 2.3.4 and Kibana 3.0.x. 13.Designing of ETL applications with Scalding and Cascading in Scala and using their core API for MapReduce 1.0 . 14.Design of new Flume agent ingestion processes, from near real-time data updates to Hadoop data sinks. 15.Big Data analytics with Cloudera CDH 4.3 and CDH5.2 in  Hadoop ecosystem using Hadoop Streaming API for Scalding and with Cascading. Usage of Scoop connections for data ingestion from Oracle dataBases, Netezza and ABN Issue . Also implement analytics Jobs  with scripting in PIG, HIVE and Impala. 16.ETL processing and BI analysis using Python UDF(s) in HIVE . 17.Restful API  implementations, for data consumption and ingestion to data sinks. ( FLUME with Plugging ingestion of HTTP consumers) Project roles :  7.Development of a new solution for ETL on Mobile visualized content. Used JSON, CSV and XML ETL and parsing tools of Cascading and Scalding.  8.Prepare and migrate data products in between Cloudera CDH4.2 and Cloudera CDH5.2. \u00a09.Daily support of production data and analytics.  \u25cfSoftware Consultant Big Data Analytics, Cloudera CDH (From November 2014 \u2013 to December 2015 )  Contractor from Gravitas for ( QUDINI ) (Contractor)  Business or sector: \u200bSystems design and migration from Cloudera CDH 4.3 to Cloudera CDH 5.2 with ETL techniques for Product content Analytics and CRM data. .  Technologies used: 18.Big Data analytics with Cloudera CDH_5.1 , Hadoop ecosystem using Scoop connections and doing analytics with scripting in PIG, HIVEQL and Impala. 19.ETL processing and BI analysis using Hive with Python UDF(s) and HIVEQL. 20.SOA implementation with J2EE framework, web Services in Struts and Spring API. Project roles :  10.Design and implementation of a new Hadoop Cluster for a Startup company, specialized in software for Restaurant Queue Management in London. Design of the solution using Cloudera CDH5.1 and implementation of packages, Hadoop, Yarn, MapReduce 2, Hive, Impala, Sqoop, HBase. \u00a011.Project not finished due to issues in Startup.  \u25cfSoftware Systems Data Analyst and Senior Software Developer / Analyst (From May 2010 \u2013 to October 2014 )  Fidelity Global, EMEA Watford (\u200bwww.fisglobal.com/EMEA/UK\u200b \u200b) (Contractor)  ImpactZero Software (\u200bwww.impactzero.pt\u200b) (Senior Software Analyst)  Business or sector: \u200bSystems Migration with ETL and CRM techniques for Core Banking, customer data and transactional data from payment systems. New data transformation processes for funds in-clearing and migration payment redirections in domestic and international schemas as FPS, BACS, CHAPS, SEPA and following standard formats SWIFT MT103 , BACS AUDIS and BACS ADDACS DDI. Product development and life cycle support for Business and Current account product.  Technologies used: \f"}, {"page_n": "5", "p_content": "21.Big Data analytics with MySQL, and MongoDB ecosystems and more recently with Hadoop ecosystem using Scoop connections and doing analytics with scripting in PIG, HIVEQL and Impala. 22.ETL processing and BI analysis using Hive with Python UDF(s) and HIVEQL . 23.SOA implementation with J2EE framework, web Services in Struts and Spring API. Project roles :  12.S\u200bystems and Data Reporting Control Analyst in projects for \u200bBarclays Direct formerly known ING Direct UK. Data Quality check for Data WareHousing, Business Intelligence and Customer Relationship management.  13.MDM \u200bAnalysis, Design of processes on business requirements with technical implementations for\u00a0Data Migration in Core Banking applications. Active participation in integration discussions and\u00a0cross-functional issues resolution control. Active participation in all the different stages of the\u00a0migration project: requirement gathering, analysis, build, implementation, system testing and\u00a0performance control. Participation on the go-live and coverage of data migration.\u00a0 14.Technical s\u200bpecialties: 4+ years of experience in Banking Software Solutions, performed different\u00a0roles and tasks, from Functional, Technical Analysis, Build and Implementation. Covered several\u00a0domains, such as Core Banking CRM, B2C/B2B, Web Services, SOAP, SOA, Transactional Data\u00a0Capture and Historical log processing. Use RPC techniques and technical solutions as :\u00a0 1.Implement xml parsers for ISO 20022 message standard and SEPA payment exchanges.  2.Develop xml parsers for real-time customer SMS, using connector IBM WebSphere MQ V7.0 client components.  3.Develop new core product processes using Web services with Struts, and Spring API for Customer bank applications using SOAP MEP patterns.  4.Develop new business account product in FIS Profile Core banking : Front-end in Java ( Web Services or SOA ) and back-end in GT-M VM, scripting in KSH for AIX console.  \u25cf \f"}, {"page_n": "6", "p_content": "Software Analyst, Developer and Tester   \u200b( From June 2008 \u2013 to April 2010)  T-Systems Iberia Barcelona / D-Core Lisbon \u200b(\u200bwww.t-systems.pt/\u200b) \u200b.  Started as a junior software analyst and developer.  Worked in SLI (Sales system) applications for VW UK and VAESA.  Business or sector :\u200b \u200bAnalysis, Development and Software Testing using ETL techniques for car sales systems.  Technologies used: 1.ETL Technologies in mainframe zOS ecosystem using COBOL, JCL and CICS. Project roles : \u00a01.Actively participate in the different stages of the project : analysis, build, testing and\u00a0implementation.\u00a0\u00a02.Actively participate in new ETL processes for Migration in between Mainframe systems ( From\u00a0VAESA in Spain to VW UK in Wolfsburg).\u00a0 3.Development of new BI and SLI logic customised to RDD of UK customer .  4.Use technologies for process control and Data : JIRA and HP Quality Centre.  5.Use technologies for data ETL: Mainframe host with COBOL, JCL, CICS  and DB2 database.  6.Development and testing control of SLI BI using Java Swing interfaces.  EDUCATION AND TRAINING   \u25cfCloudera Data Science Training . London UK : ( 2016 - ) 1.The fundamentals of Cloudera Big Data Lake. 2.Fundamentals of Data Science with Python, and R. 3.Practical use case analytics using ALS recommendation model. 4.Comparative analysis of models using R plotting, via ggplot2 and rCharts. \u25cfCloudera Administration Training. London UK : ( 2016 - ) 5.The fundamentals of Cloudera Big Data Lake. 6.Setup of a 5 Nodes Cluster using AWS and EMR. 7.Scale of a Cluster and setup of Services in a Cluster. 8.Root cause analysis for issues and problems in a cluster.  9.Scalability in a CDH cluster, adjust Dynamic Pool of resources and Static Pool of resources. \u25cfCloudera Data Analyst Training. \u200bLondon UK : (2014 - )  10.The fundamentals of Apache Hadoop and data ETL (extract, transform, load), ingestion, and processing with Hadoop MapReduce and tools  11.Organizing data into tables, performing transformations, and simplifying  complex queries with Hive  12.Joining multiple data sets and analysing disparate data with Pig  13.Performing real-time interactive analyses on massive data sets stored in  HDFS or HBase using SQL with Impala.  14.Connectivity of Hadoop HDFS and Oracle 11g or MySQL using Sqoop.   \f"}, {"page_n": "7", "p_content": "\u25cfContinuous Training in FIS Profile Technologies. \u200bFIS \u2013 Fidelity Global : \u200b(2010 - 2014)   1.Training in Core Banking B.I. -  Data ETL and CRM . Usage of Falcon Fico , Experian extraction and IBM Cognos. 2.Training in Core Banking technical solutions from FIS Global.  3.Currently also doing training for junior resources on-site and off-shore.   \u25cfTraining in z-OS Mainframe Technologies.\u200bT-Systems - IBM Portugal\u200b : \u200b(2008 - 2009)  Training for integration in T-Systems project. IBM Mainframe z/OS modules :  \u25cfES10 \u2013 Fundamental System Skills in Z/OS.  \u25cfAD40 \u2013 COBOL Programming Fundamentals.  \u25cfCF82 \u2013 DB2 Programming Workshop for Z/OS.  \u25cfCI01 \u2013 CICS Fundamentals.  \u25cfCI17 \u2013 CICS Application Programming I.  \u25cfCF96 \u2013 DB2 for z/OS Application Performance and Tuning.     \u25cfStudies in Mathematics and Computer Science  Universidade Do Minho, Braga Portugal : \u200b(2002 - 2007) Studies this Mathematics and Computer Science 5 year plan. An pre-Bologna Education Agreement plan with Studies in Mathematics and Advanced Software Engineering. It included advanced imperative and functional language paradigms, knowledge representation, artificial intelligence and modern distributed systems architectures. During this period have learned to work in Unix systems and software development in technologies JAVA6, Scala, Python, R, Bash, Perl, Erlang, Haskell, C, C++ . The program of degree was highly oriented to project development from requirement gathering and analysis to implementation and product implementation and ownership. PERSONAL SKILLS  Mother tongue(s) \u200bPortuguese  Other language(s) UNDERSTANDING  SPEAKING  WRITING  Listening  Reading  Spoken interaction  Spoken production  English C2 C2 C2 C2 C2 Certificate Spanish B2 B2 B2 B2 B1 Certificate Levels: A1/2: Basic user - B1/2: Independent user - C1/2 Proficient user Common European Framework of Reference for Languages   Communication skills  Good communication skills gained through my experience as a resource on customer and representing the company in requirement gathering meetings, major problem resolution meetings and in continuous support meetings.  Organisational / managerial skills  Leadership positions : -For Perform Media Group (now DAZN Group) was the Head of BigData. I did mentoring of teams in the space \f"}, {"page_n": "8", "p_content": "of Big Data technologies, conducted team knowledge refresh sessions and conducted the adoption of Big Data frameworks like Spark, managed near-shore teams in Poland (Katowice) and in Slovakia (Kosice). -For FISGlobal / ING Direct did team integration via phone and video conference calls, as currently responsible for a small team of 5 people in between on-site (UK Reading) and off-shore resources (India Bangalore).  Job-related skills  Highly skilled in software development life cycle, for Media systems in the big data stack also before in the maintenance, support, builds and packaging of core banking software solutions (responsible in FIS team for software life cycle maintenance and production deployment, including migration and services closure).  Computer skills  Full domain of JIRA Agile Project management tool. Full domain of test management HP Quality Center. Full domain of Microsoft Office tools as Visio, Excel, Word and Project. Software development with in SOA, Java Web Services, MySQL, PostgreSQL, banking product development. Also knowledge in scripting for UNIX / AIX and Z/OS systems using Bash, KSH and JLC.  Other skills  Storytelling skills / Soft skills, capable to communicate ideas and manage the art of communication in holistic human side. Worked in the family business (in accountancy and stock management areas). Cyclist on free weekends in both road and mountain, also a daily office commute when cycling is possible.  Driving licence  Driving Licence, category B  \f"}, {"page_n": "9", "p_content": "TrainerAntonio Radescadate of issue:2019-06-242019-06-24has been presented toJoao Cerqeuirafor the completion ofDomain Driven Design - bespokeVenueLondonLondonDuration1 Day1 Daye-certificate: https://cert.nobleprog.com/authenticateCertificate ID: 586523Authentication Code: cde51\f"}, {"page_n": "10", "p_content": "Joao CerqueiraCongratulations! Through your contributions to the community, you have earned the Green Ribbon badge.We truly appreciate you sharing your knowledge and expertise with the community,and we look forward to seeing even more of your valued posts!January 01, 2019\f"}, {"page_n": "11", "p_content": "Certificate of Attendance is hereby granted to   To verify that he/she has attended Data Science at Scale  using Spark and Hadoop   Cloudera, Inc. www.cloudera.com  ___________________________ VP, Educational Services ___________________________ Course Date\tJoao CerqueiraOctober 5th 2016\f"}, {"page_n": "12", "p_content": "Certificate of Attendanceis hereby granted toJoao CerqueriaTo verify that he/she has attendedCloudera Data Analyst Training:Using Pig, Hive, and Impala with HadoopCloudera, Inc.www.cloudera.com ___________________________VP, Educational Services___________________________DateAugust 12-14th9AM-5PM\f"}, {"page_n": "13", "p_content": "\f"}, {"page_n": "14", "p_content": "\f"}]}