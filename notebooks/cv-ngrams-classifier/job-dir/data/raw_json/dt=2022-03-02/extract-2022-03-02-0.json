{"filename": "CV-JoaoCerqueira-2021-2022-withcerts", "pages": [{"page_n": "1", "p_content": "Data Architectures, Data Engineering, Data Science and Cloud Migrations expertPersonal InformationName:Jo\u00e3o Pedro Afonso CerqueiraSex:MaleDate of birth:19/12/1983Address UK:21, Woodlands Avenue, Worcester Park,KT47ALContacts:(+44) 07572550311Email:joao@fuelbigdata.com( alternativejpacerqueira@gmail.com)Skype:jpacerqueira83Linkedin:uk.linkedin.com/in/joaopedroafonsocerqueiragithub:github.com/jpacerqueirapersonal website and blog :https://fuelbigdata.comProfessional Summary\u25cfHead and/or VP of Data Engineering in previous companies since 2017\u25cfDefine programs of engineering for Cloud solutions like:\u25cbAWS Cloud Migration from on-prem\u25cbGCP Cloud Migration and transformation\u25cbModernization of Data stack with Data Mesh\u25cbModernization of Data stack with Data Fabric\u25cfBig Data / Analytics and Data Science expert with +13 years in the market\u25cfData Architect with hands-on experience establishing solutions for reference architectures\u25cbData Mesh\u25cbData Factory\u25cbLambda Architecture & Kappa Architecture\u25cfExpert in technology stacks comprising Data Analytics, Warehousing, Data Modelling,ETL/ELT,  Visualisation, Data Science, Data Analytics\u25cfExperience in the 3 cloud practices (AWS, GCP, Azure) and its full-stack analytics solutions.\u25cfExperience in Data Analytics solutions with Spark, Databricks, Google DataProc, AWSEMR, Cloudera CDP\u25cfExperience in hybrid and on-premises stacks Hadoop, Spark, Cloudera CDH\u25cfCloudera Data Analytics, Data Science and Architecture certifications and training.\u25cfAWS Data Analytics certifications ongoing\u25cfGCP Data Professional certification ongoing\u25cfExperience in Kubernetes ecosystems in Azure AKS and tpoc for GCP GKE, and AWSEKS.Work Experience\u25cfHead of Data - VP Data Architecture - Cloud Data Solutions - Data Consultancy Services -FirstDerivative ( non-KX Data and investment banking)(From Sept.2021 - current Date)\f"}, {"page_n": "2", "p_content": "Business or sector:Consultancy services via First Derivative, for financial services institutions, across multiplemarkets, divided by regions  NAM and EMEA.Consultancy implementation:\u25cfDefined the Data Engineering strategy for the company and approach to scale the Data Engineering team.\u25cfDefine use cases for consultancy offerings and application of Data Engineering patterns across business andtechnology functions, including :\u25e6Data Mesh, and product driven design of solutions\u25e6Data Fabric, and centrally managed cloud warehouse and modern solutions\u25cfImplement an roadmap for team growth and team scaling across the board\u25cfImplement a program of graduate training and onboarding into Data Engineering functions.Technology implementation:\u25cfSolutions and Data Architecture consultancy services via FD, for financial services customers in TorontoCanada from London UK.\u25cfConsulted during a discovery phase at the customer, via  analysis of systems in-order to facilitate a porting ofservices from on-prem to cloud.\u25cfSolution Architecture and Data Architecture design proposal, using fpML industry solution for trading.\u25cfDesign of new target Architecture following Data Mesh and Data Streaming patterns with:\u25cbEvent Sourcing and Event Streaming patterns\u25cbSaga Patterns and aggregate/state communications between Domains and/or Services.\u25cbKubernetes in AWS EKS\u25cbConfluent Kafka\u25cbKafka Adaptors Java/Python\u25cbMicro-services\u25a0Java K8S microservices and/or\u25a0alternative proposal and TPOC , AWS Lambda Python microservices\u25cbCI/CD in AWS Code Pipelines TPOC\u25cfSolution didn\u2019t advance to implementation, due to customer/budget constraints.\u25cfLead VP Data Architecture - Data Unit - 6point6.co.uk ( private sector customers )(From Oct.2020 - Aug.2021)Business or sector:Consultancy services via 6point6Limited UK, for financial services customers in the city ofLondon.Consultancy implementation:\u25cfMigration  Solution design into AWS Serverless solutions\u25e6Analysis of on-prem environment, setup decommissioning strategies to cloud migration\u25e6Planning of migration into AWS solutions compatible with use casesTechnologies used:\u25cfDesign of new target Architecture AWS EMR  and AWS Glue Serverless :\u25e6AWS Code Pipelines\u25e6AWS Lambda + Step functions + AWS Glue\u25e6AWS Lambda + Step functions + AWS EMR\u25e6AWS Athena\u25e6AWS Kinesis, Kinesis Firehose\u25e6Tableau and AWS QuickSight\u25cfOn-prem solution analysis, and decommissioned solutions :\u25e6Hortonworks HDP, Spark 2, Hadoop, Hive\f"}, {"page_n": "3", "p_content": "\u25e6Jenkins\u25e6Jupyter, Python pandas, PostgreSQL,\u25e6Apache Kafka and Apache Spark with Spark Streaming.\u25cfLead Data Consultant - Data Science and Data Engineering - GFT Financial Limited(From June2019 - Sept.2020)GFT Financial Limited  - City of London (https://www.gft.com/uk/en/index/) (Permanent)Business or sector:Consultancy services via GFT FinancialLimited UK, for non-financial services customers.Technologies used:1.Design and Implementation of an Collection Tool, for a data solution in a client using :1.Azure DevOps Pipelines2.Terraform for Azure SQL DW ( PostgreSQL), Azure AKS (Kubernetes  + Java jgroup solution)3.Kubernetes AKS4.Azure PostgreSQL, multi-region Citus cloud5.Data Mesh solution, with curation and cataloguing, with UI and API build with Activiti + CQRS + GraphQL .2.Design and implement a Multi-Cloud solution for Data Science ML & Analytics, focus on Python solutions.3.Setup of ecosystems and Tools in an Hadoop Distributed ecosystem, including :1.Apache Hadoop, Hive, Zookeeper, Kafka, Spark, HBase.2.Setup of Spark Streaming ecosystems on Kafka+Spark+Hadoop(permanent persistence) ecosystems.4.Setup of a containerized Data Science ecosystems using docker containers :1.Spark latest version up to 2.4.5  and Delta Lake.2.Establish of DataOps landing zones for prototypes Multi-cloud and private Cloud.5.Establishment of consultancy with GFT using the principles defined by my Fuel BigData entity.1.An entity dedicated  to Data Engineering and Data Science consultancy.2.Establish a public contributor repo for democratic usage of the solution.1.Detials inhttps://github.com/jpacerqueira/Jupyter_Spark_H2O_Kafka_Client_Setup3.Applied all my skills in full stack Data Engineering, Analytics , Data Science with ML tools for CI/CD buildsand continuous deployments.1.Solutions in detail inhttps://fuelbigdata.com4.Establishment of Container solutions with Jupyter Notebooks and tools1.Used in practical investigations for worldwide propagation ofhttps://fuelbigdata.com/?blogcategory=Covid19+InvestigationProject roles :1.Got involved in new customer engagement, supporting new sales pitches for new business areas and markets.Establish principles and define general technical solution approaches for Data Science in a Multi-cloudenvironment (GCP, AWS and Azure).2.Lead the first Azure Kubernetes project, with implementation of a solution with security principles, azure secretmanagement and CI/CD Azure Pipelines for its deployment.3.Lead Data Architect for an UK customer outside Financial Services, customer in \u201cProperty Sector\u201d . Establishthe technical solution with their new Technology department. Collect information and define their new solutionfor Business Intelligence and Data Collection at the business level. Implement the architecture solution in\f"}, {"page_n": "4", "p_content": "Azure Cloud using services of Azure SQL with Citus PostGreSQL and an AKS Kubernetes infrastructure tocollect, exchange and service information to partners, via a WebApp solution ( Alfresco API Java +TypeScript_JS ). Define a General approach for Analytics for derived applications, with ETL supported usingPython and PySpark with Apache Arrow accelerators. Solution at full-stack TPOC level was customerapproved and handed over for development with nearshores.\u25cfBig Data & Data Science  Architect - Head of Big Data - Perform Media Group(From November 2015 \u2013 June2019 )DAZN Media Group ( ex- Perform Media)  - London (http://www.performgroup.com/contact/)(Permanent)Business or sector:Perform Group is a Sports Mediagroup owner of Brands: Opta, Runningball, Goal.com,SportingNew.com and DAZN.com ( OTT b2c video subscription).Technologies used:6.Design and implement the Big Data full stack at Perform Group using the following technologies:1.Cloudera Enterprise Data Hub ( PaaS ) , CDH 5.9 and recently CDH 5.14 .2.Oracle enterprise Big Data Appliance ( BDA) , with Cloudera CDH.3.Implemented the new generation of IOT collection from tracking data supply of tracking cameras to enrichOpta event data. Usage of Akka collection tools using Fast Data principles and Kafka buffer storage forSpark Streaming analytics.4.Implemented a \u201cSingle Customer View\u201d across the Portals of Perform Group , goal.com sportingnew.com ,dazn.com , dabblebet.com and others to exchange customer profiles and unique_id between DW,Marketing Cloud CRM and individual site profiles. Designed a solution using Neo4J for graph calculationand Big Data Cloudera CDH for log analysis.7.Design of a new AWS Cloud solution for DW for DAZN product using AWS EMR, RedShift, RedShiftSpectrum, AWS Glue Crawlers.1.Design the new generations of S3 lake based on Object Store.2.Design the new generation of MetaStore collection using Schema on read over object store with AWSGlue Crawlers + AWS EMR + AWS Databricks3.Enable the reception of Media Portal profiles and customers in the new S3 Lake, from the on-premisesolution designed as Single Customer View for Perform Media Services.8.Designing a New general framework for ETL Solutions based on:1.Control of /raw /staged /published areas2.Utilize ETL technologies Apache Spark 1.6.0/2.1.0/2.3.0 ( Python and Scala)3.Achieve published insights for apache Hive.4.Served insights , on-premise using Oracle Big Data SQL.5.Served insights, in-cloud AWS with using RedShift Spectrum.9.Full setup of Data Science Stack for Brands Opta and Perform Media1.Used technologies based on Python and R languages for data science.1.R-Studio and R as a data science stack of the new generation of Perform logic.2.Setup of services with Oracle R Enterprise (ORE dbi) for ORACLE DB.3.Setup for  BigData with Hadoop/Hive using ORCH R/cran.1.Design of processes and setup of services for the Data Science team, to align in the SDLC ofdevelopment teams.2.Setup of data science services with sparklyr and tools of a new generation of machine learningand Deep learning services using R H20.ai .4.Setup for  BigData with Hadoop/Hive using PySpark.\f"}, {"page_n": "5", "p_content": "1.Design of processes and setup of services for the Data Science team, to align in the SDLC ofdevelopment teams.2.Setup of DataScience stack with pyspark, sparkling water, h2o.ai services and tools for a newgeneration of machine learning and deep learning services using Python H20.ai.10.Opta services, setup of SOLR search from collected Sports Data, and sports fixtures ( uuids for sports venues,players, clubs, leagues, tournaments, seasons) .11.Setup of Oracle Big Data Discovery visual tool over on-premissine BDA cluster.1.Usage of lake with published Apache Hive in Spark context jobs.12.Setup of H2o Deeplearning and Boosting solutions, using R.1.Implementation of decision pipelines for content distribution for DAZN.1.A solution T-POC for Streaming AVGBitRate evaluation of normal/abnormal conditions, inhttp://bit.ly/2nFHYpf.2.A solution T-POC for Malware/Phishing intrusion detection and prediction. Designed with url basedevaluation , based on external/internal data and conditions, inhttp://bit.ly/2qAISok.13.Establishment of Fuel BigData .1.An entity dedicated for Ai consultancy.2.Follow site for more detailshttps://fuelbigdata.com.Project roles :4.Lead Architect for the new Sports Media IOT solution , responsible for ingestion of third party IOT trackingdata, being matched with internal Generated OptaSports event data generated in NRT. Production of newgeneration of Sports insights, from an event driven to an IOT driven solution, where new insights wereproduced around enriched event data. Responsible for the IOT tracking software R&D, Development of newpipelines and Data Science of new algorithms and tools. Represented Perform Group at the OracleOpenWorld 2017 in Intel Keynote sessions where our BDA solution was presented with our partner Oracle (youtube link) . Presented the project in a varietyof Oracle and Cloudera events (cloudera link) in2017 and2018.5.Lead Architect for BigData solution for the new generation of Media Data for Sports insights across socialmedia, designed and implementation of the social media collection tools, using Data streams from Omniture,Google Analytics , Google Ads (ex- Google DFP), Gygia and other social media l registrations for Facebook,Twitter and other social media portals. Implementation of Google Analytics, AdWord solution and Google Adsfor market understanding and segmentation. Implementation of a new generation of Marketing Cloud DMP, forGoal.com and DAZN.com brands using Bluekai from Oracle Cloud. Full deployment and end to end control ofsolution activation and reporting using tools such Oracle BDA BigData and Oracle Bluekai cloud solutions.6.Lead Architect for a solution for Sports integrity for Betting. Usage of Oracle Big Data SQL ( 3.1) for dataservicing of a new generation of Spark/ML and SparkLyR context analysis over sports Data. Publication ofdata from Hive database , with transformations over collected data in SOLR search, for abnormal patterndetection. Solution TPOC expanded into an AWS Solution further in the pipeline.7.Lead Architect for the new DAZN EDW solutions, coordinating the teams to scale a new platform for AWSCloud with Big Data using solutions, AWS Data Lake formation with AWS Glue, AWS RedShift and Spectrum,AWS EMR, AWS Kinesis. Solution further expanded on my recommendation with Snowflake Data Warehousein AWS as elastic/scalable replacement for Redshift.\u25cfSenior Software Developer - EDM Big Data(From January 2015 \u2013 October 2015 )SKY , CBS - EDM Osterley (https://corporate.sky.com/contact-us) (Permanent)Business or sector:Systems Migration from ClouderaCDH 4.3 to Cloudera CDH 5.2 with ETL and CRM techniquesfor Media and Core product content Analytics.Technologies used:14.Designing ETL Solutions with Apache Spark 1.3 and production of insights for ElasticSearch 2.3.4 and Kibana\f"}, {"page_n": "6", "p_content": "3.0.x.15.Designing of ETL applications with Scalding and Cascading in Scala and using their core API for MapReduce1.0 .16.Design of new Flume agent ingestion processes, from near real-time data updates to Hadoop data sinks.17.Big Data analytics with Cloudera CDH 4.3 and CDH5.2 in Hadoop ecosystem using Hadoop Streaming API forScalding and with Cascading. Usage of Scoop connections for data ingestion from Oracle dataBases, Netezzaand ABN Issue . Also implement analytics Jobs  with scripting in PIG, HIVE and Impala.18.ETL processing and BI analysis using Python UDF(s) in HIVE .19.Restful API  implementations, for data consumption and ingestion to data sinks. ( FLUME with Pluggingingestion of HTTP consumers)Project roles :8.Development of a new solution for ETL on Mobile visualized content. Used JSON, CSV and XML ETL andparsing tools of Cascading and Scalding.9.Prepare and migrate data products in between Cloudera CDH4.2 and Cloudera CDH5.2.10.Daily support of production data and analytics.\u25cfSoftware Consultant Big Data Analytics, Cloudera CDH(From November 2014 \u2013 to December 2015 )Contractor from Gravitas for ( QUDINI ) (Contractor)Business or sector:Systems design and migration fromCloudera CDH 4.3 to Cloudera CDH 5.2 with ETLtechniques for Product content Analytics and CRM data. .Technologies used:20.Big Data analytics with Cloudera CDH_5.1 , Hadoop ecosystem using Scoop connections and doing analyticswith scripting in PIG, HIVEQL and Impala.21.ETL processing and BI analysis using Hive with Python UDF(s) and HIVEQL.22.SOA implementation with J2EE framework, web Services in Struts and Spring API.Project roles :11.Design and implementation of a new Hadoop Cluster for a Startup company, specialized in software forRestaurant Queue Management in London. Design of the solution using Cloudera CDH5.1 andimplementation of packages, Hadoop, Yarn, MapReduce 2, Hive, Impala, Sqoop, HBase.12.Project not \ufb01nished due to issues in Startup.\u25cfSoftware Systems Data Analyst and Senior Software Developer / Analyst(From May 2010 \u2013 to October 2014 )Fidelity Global, EMEA Watford (www.fisglobal.com/EMEA/UK) (Contractor)ImpactZero Software (www.impactzero.pt) (Senior SoftwareAnalyst)Business or sector:Systems Migration with ETL andCRM techniques for Core Banking, customer data andtransactional data from payment systems. New data transformation processes for funds in-clearing and migrationpayment redirections in domestic and international schemas as FPS, BACS, CHAPS, SEPA and following standardformats SWIFT MT103 , BACS AUDIS and BACS ADDACS DDI. Product development and life cycle support forBusiness and Current account products.Technologies used:23.Primarily SOA implementation with J2EE framework, web Services in Struts and Spring API.24.For migrations also some Big Data analytics with PostgreSQL, MySQL, MongoDB first and further expansionin 2013/2014 with Hadoop/Hive ecosystems using Sqoop connectors to relational systems , to do analytics\f"}, {"page_n": "7", "p_content": "with scripting in PIG, HIVEQL and Scala/Scalding.25.ETL processing and BI analysis using Hive with Python UDF(s) and HIVEQL .Project roles :13.Systems and Data Reporting Control Analyst in projectsforBarclays Direct formerly known ING Direct UK.Data Quality check for Data WareHousing, Business Intelligence and Customer Relationship management.14.MDMAnalysis, Design of processes on business requirementswith technical implementations for DataMigration in Core Banking applications. Active participation in integration discussions and cross-functionalissues resolution control. Active participation in all the di\ufb00erent stages of the migration project: requirementgathering, analysis, build, implementation, system testing and performance control. Participation on thego-live and coverage of data migration.15.Technical specialties: 4+ years of experience in BankingSoftware Solutions, performed di\ufb00erent roles andtasks, from Functional, Technical Analysis, Build and Implementation. Covered several domains, such as CoreBanking CRM, B2C/B2B, Web Services, SOAP, SOA, Transactional Data Capture and Historical logprocessing. Use RPC techniques and technical solutions as :1.Implement xml parsers for ISO 20022 message standard and SEPA payment exchanges.2.Develop xml parsers for real-time customer SMS, using connector IBM WebSphere MQ V7.0 clientcomponents.3.Develop new core product processes using Web services with Struts, and Spring API for Customerbank applications using SOAP MEP patterns.4.Develop new business account product in FIS Profile Core banking : Front-end in Java ( Web Servicesor SOA ) and back-end in GT-M VM, scripting in KSH for AIX console.\u25cfSoftware Analyst, Developer and Tester( From June 2008 \u2013 to April 2010)T-Systems Iberia Barcelona / D-Core Lisbon(www.t-systems.pt/).Started as a junior software analyst and developer.Worked in SLI (Sales system) applications for VW UK and VAESA.Business or sector :Analysis, Development and SoftwareTesting using ETL techniques for car sales systems.Technologies used:1.ETL Technologies in the Mainframe zOS ecosystem using COBOL, JCL and CICS.Project roles :1.Actively participate in the di\ufb00erent stages of the project : analysis, build, testing and implementation.2.Actively participate in new ETL processes for Migration in between Mainframe systems ( From VAESA inSpain to VW UK in Wolfsburg).3.Development of new BI and SLI logic customised to RDD of UK customers (VW-UK) .4.Use technologies for process control and Data : JIRA and HP Quality Centre.5.Use technologies for data ETL: Mainframe host with COBOL, JCL, CICSand DB2 database.6.Development and testing control of SLI BI using Java Swing interfaces.EDUCATION AND TRAINING\u25cfCloudera Data Science Training . London UK : ( 2016 - )\f"}, {"page_n": "8", "p_content": "1.The fundamentals of Cloudera Big Data Lake.2.Fundamentals of Data Science with Python, and R.3.Practical use case analytics using ALS recommendation model.4.Comparative analysis of models using R plotting, via ggplot2 and rCharts.\u25cfCloudera Administration Training. London UK : ( 2016 - )5.The fundamentals of Cloudera Big Data Lake.6.Setup of a 5 Nodes Cluster using AWS and EMR.7.Scale of a Cluster and setup of Services in a Cluster.8.Root cause analysis for issues and problems in a cluster.9.Scalability in a CDH cluster, adjust Dynamic Pool of resources and Static Pool of resources.\u25cfCloudera Data Analyst Training.London UK : (2014- )10.The fundamentals of Apache Hadoop and data ETL (extract, transform, load), ingestion, and processing withHadoop MapReduce and tools11.Organizing data into tables, performing transformations, and simplifyingcomplex queries with Hive12.Joining multiple data sets and analysing disparate data with Pig13.Performing real-time interactive analyses on massive data sets stored inHDFS or HBase using SQL with Impala.14.Connectivity of Hadoop HDFS and Oracle 11g or MySQL using Sqoop.\u25cfContinuous Training in FIS Profile Technologies.FIS\u2013 Fidelity Global :(2010 - 2014)1.Training in Core Banking B.I. -  Data ETL and CRM . Usage of Falcon Fico , Experian extraction and IBMCognos.2.Training in Core Banking technical solutions from FIS Global.3.Currently also doing training for junior resources on-site and off-shore.\u25cfTraining in z-OS Mainframe Technologies.T-Systems- IBM Portugal:(2008 - 2009)Training for integration in T-Systems project. IBM Mainframe z/OS modules :\u25cfES10 \u2013 Fundamental System Skills in Z/OS.\u25cfAD40 \u2013 COBOL Programming Fundamentals.\u25cfCF82 \u2013 DB2 Programming Workshop for Z/OS.\u25cfCI01 \u2013 CICS Fundamentals.\u25cfCI17 \u2013 CICS Application Programming I.\u25cfCF96 \u2013 DB2 for z/OS Application Performance and Tuning.\u25cfStudies in Mathematics and Computer ScienceUniversidade Do Minho, Braga Portugal :(2002 - 2007)Studies this Mathematics and Computer Science 5 year plan. An pre-Bologna Education Agreement plan with Studiesin Mathematics and Advanced Software Engineering. It included advanced imperative and functional language\f"}, {"page_n": "9", "p_content": "paradigms, knowledge representation, artificial intelligence and modern distributed systems architectures. During thisperiod have learned to work in Unix systems and software development in technologies JAVA6, Scala, Python, R,Bash, Perl, Erlang, Haskell, C, C++ . The program of degree was highly oriented to project development fromrequirement gathering and analysis to implementation and product implementation and ownership.PERSONAL SKILLSMother tongue(s)PortugueseOther language(s)UNDERSTANDINGSPEAKINGWRITINGListeningReadingSpoken interactionSpokenproductionEnglishC2C2C2C2C2CertificateSpanishB2B2B2B2B1CertificateLevels: A1/2: Basic user - B1/2: Independent user - C1/2 Proficient userCommon European Framework of Reference for LanguagesCommunication skillsGood communication skills gained through my experience as a resource on customer and representing the company in requirementgathering meetings, major problem resolution meetings and in continuous support meetings.Organisational / managerial skillsLeadership positions :-For GFT Group UK - Lead of a first Azure Kubernetes project, with implementation of solution with security principles andCI/CD Azure Pipelines. Coordinate the team expansion and growth with an NearShore in Poland.-For Perform Media Group (now DAZN Group) was the Head of BigData. I did mentoring of teams in the space of Big Datatechnologies, conducted team knowledge refresh sessions and conducted the adoption of Big Data frameworks like Spark,managed near-shore teams in Poland (Katowice) and in Slovakia (Kosice).-For FISGlobal / ING Direct did team integration via phone and video conference calls, as currently responsible for a smallteam of 5 people in between on-site (UK Reading) and off-shore resources (India Bangalore).Job-related skillsHighly skilled in software development life cycle, for Media systems in the big data stack also before in the maintenance, support,builds and packaging of core banking software solutions (responsible in FIS team for software life cycle maintenance andproduction deployment, including migration and services closure).Computer skillsFull domain of JIRA Agile Project management tool. Full domain of test management HP Quality Center. Full domain of MicrosoftOffice tools as Visio, Excel, Word and Project. Software development with in SOA, Java Web Services, MySQL, PostgreSQL,banking product development. Also knowledge in scripting for UNIX / AIX and Z/OS systems using Bash, KSH and JLC.Other skillsStorytelling skills / Soft skills, capable to communicate ideas and manage the art of communication in holistic human side. Workedin the family business (in accountancy and stock management areas). Cyclist on free weekends in both road and mountain, also adaily office commute when cycling is possible.Driving licenceDriving Licence, category B\f"}, {"page_n": "10", "p_content": "(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:7)(cid:5)(cid:8)(cid:9)(cid:5)(cid:4)(cid:9)(cid:10)(cid:11)(cid:3)(cid:12)(cid:6)(cid:13)(cid:14)(cid:3)(cid:12)(cid:3)(cid:8)(cid:6)(cid:10)(cid:9)(cid:15)(cid:10)(cid:4)(cid:13)(cid:13)(cid:16)(cid:6)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:19)(cid:23)(cid:22)(cid:18)(cid:24)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:19)(cid:23)(cid:22)(cid:18)(cid:24)(cid:25)(cid:3)(cid:13)(cid:10)(cid:26)(cid:6)(cid:6)(cid:5)(cid:10)(cid:27)(cid:2)(cid:6)(cid:13)(cid:6)(cid:5)(cid:8)(cid:6)(cid:12)(cid:10)(cid:8)(cid:9)(cid:28)(cid:9)(cid:3)(cid:9)(cid:10)(cid:29)(cid:6)(cid:2)(cid:30)(cid:6)(cid:16)(cid:4)(cid:2)(cid:3)(cid:15)(cid:9)(cid:2)(cid:10)(cid:8)(cid:25)(cid:6)(cid:10)(cid:14)(cid:9)(cid:31)(cid:27)(cid:32)(cid:6)(cid:8)(cid:4)(cid:9)(cid:5)(cid:10)(cid:9)(cid:15)(cid:33)(cid:9)(cid:31)(cid:3)(cid:4)(cid:5)(cid:10)(cid:33)(cid:2)(cid:4)(cid:34)(cid:6)(cid:5)(cid:10)(cid:33)(cid:6)(cid:13)(cid:4)(cid:35)(cid:5)(cid:10)(cid:22)(cid:10)(cid:26)(cid:6)(cid:13)(cid:27)(cid:9)(cid:36)(cid:6)(cid:37)(cid:6)(cid:5)(cid:16)(cid:6)(cid:38)(cid:9)(cid:5)(cid:12)(cid:9)(cid:5)(cid:38)(cid:9)(cid:5)(cid:12)(cid:9)(cid:5)(cid:33)(cid:16)(cid:2)(cid:3)(cid:8)(cid:4)(cid:9)(cid:5)(cid:20)(cid:10)(cid:33)(cid:3)(cid:39)(cid:20)(cid:10)(cid:33)(cid:3)(cid:39)(cid:6)(cid:22)(cid:14)(cid:6)(cid:2)(cid:8)(cid:4)(cid:15)(cid:4)(cid:14)(cid:3)(cid:8)(cid:6)(cid:17)(cid:10)(cid:25)(cid:8)(cid:8)(cid:27)(cid:13)(cid:17)(cid:40)(cid:40)(cid:14)(cid:6)(cid:2)(cid:8)(cid:41)(cid:5)(cid:9)(cid:26)(cid:32)(cid:6)(cid:27)(cid:2)(cid:9)(cid:35)(cid:41)(cid:14)(cid:9)(cid:31)(cid:40)(cid:3)(cid:16)(cid:8)(cid:25)(cid:6)(cid:5)(cid:8)(cid:4)(cid:14)(cid:3)(cid:8)(cid:6)(cid:29)(cid:6)(cid:2)(cid:8)(cid:4)(cid:15)(cid:4)(cid:14)(cid:3)(cid:8)(cid:6)(cid:10)(cid:42)(cid:33)(cid:17)(cid:10)(cid:43)(cid:44)(cid:23)(cid:43)(cid:18)(cid:45)(cid:7)(cid:16)(cid:8)(cid:25)(cid:6)(cid:5)(cid:8)(cid:4)(cid:14)(cid:3)(cid:8)(cid:4)(cid:9)(cid:5)(cid:10)(cid:29)(cid:9)(cid:12)(cid:6)(cid:17)(cid:10)(cid:14)(cid:12)(cid:6)(cid:43)(cid:20)\f"}, {"page_n": "11", "p_content": "Certificate of Attendance is hereby granted to   To verify that he/she has attended Data Science at Scale  using Spark and Hadoop   Cloudera, Inc. www.cloudera.com  ___________________________ VP, Educational Services ___________________________ Course Date\tJoao CerqueiraOctober 5th 2016\f"}, {"page_n": "12", "p_content": "(cid:38)(cid:72)(cid:85)(cid:87)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:36)(cid:87)(cid:87)(cid:72)(cid:81)(cid:71)(cid:68)(cid:81)(cid:70)(cid:72)(cid:76)(cid:86)(cid:3)(cid:75)(cid:72)(cid:85)(cid:72)(cid:69)(cid:92)(cid:3)(cid:74)(cid:85)(cid:68)(cid:81)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:45)(cid:82)(cid:68)(cid:82)(cid:3)(cid:38)(cid:72)(cid:85)(cid:84)(cid:88)(cid:72)(cid:85)(cid:76)(cid:68)(cid:55)(cid:82)(cid:3)(cid:89)(cid:72)(cid:85)(cid:76)(cid:73)(cid:92)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:75)(cid:72)(cid:18)(cid:86)(cid:75)(cid:72)(cid:3)(cid:75)(cid:68)(cid:86)(cid:3)(cid:68)(cid:87)(cid:87)(cid:72)(cid:81)(cid:71)(cid:72)(cid:71)(cid:38)(cid:79)(cid:82)(cid:88)(cid:71)(cid:72)(cid:85)(cid:68)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:36)(cid:81)(cid:68)(cid:79)(cid:92)(cid:86)(cid:87)(cid:3)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:29)(cid:56)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3)(cid:51)(cid:76)(cid:74)(cid:15)(cid:3)(cid:43)(cid:76)(cid:89)(cid:72)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:44)(cid:80)(cid:83)(cid:68)(cid:79)(cid:68)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:43)(cid:68)(cid:71)(cid:82)(cid:82)(cid:83)(cid:38)(cid:79)(cid:82)(cid:88)(cid:71)(cid:72)(cid:85)(cid:68)(cid:15)(cid:3)(cid:44)(cid:81)(cid:70)(cid:17)(cid:90)(cid:90)(cid:90)(cid:17)(cid:70)(cid:79)(cid:82)(cid:88)(cid:71)(cid:72)(cid:85)(cid:68)(cid:17)(cid:70)(cid:82)(cid:80)(cid:3)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:57)(cid:51)(cid:15)(cid:3)(cid:40)(cid:71)(cid:88)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:54)(cid:72)(cid:85)(cid:89)(cid:76)(cid:70)(cid:72)(cid:86)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:66)(cid:39)(cid:68)(cid:87)(cid:72)(cid:36)(cid:88)(cid:74)(cid:88)(cid:86)(cid:87)(cid:3)(cid:20)(cid:21)(cid:16)(cid:20)(cid:23)(cid:87)(cid:75)(cid:28)(cid:36)(cid:48)(cid:16)(cid:24)(cid:51)(cid:48)\f"}, {"page_n": "13", "p_content": "Joao CerqueiraCongratulations! Through your contributions to the community, you have earned the Green Ribbon badge.We truly appreciate you sharing your knowledge and expertise with the community,and we look forward to seeing even more of your valued posts!January 01, 2019\f"}, {"page_n": "14", "p_content": "\f"}, {"page_n": "15", "p_content": "\f"}]}