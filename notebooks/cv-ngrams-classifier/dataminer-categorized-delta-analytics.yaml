#
# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Support for Python is experimental, and requires building SNAPSHOT image of Apache Spark,
# with `imagePullPolicy` set to Always

apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: dataminer-categorized-delta-analytics-py
  namespace: default
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "docker.io/jpacerqueira83/pyspark-k8s:2.4.5"
  imagePullPolicy: Always
#
# missing Volume
  volumeMounts:
    - mountPath: //job-dir
      name: local-jobs
    - mountPath: //job-dir/dataminer-categorized-delta-analytics.py
      name: spark-application-file
  volumes:
    - name: local-jobs
      hostPath:
        path: /Users/joaocerqueira/Documents/github/spark-on-kubernetes/jobs/job-dir   # $PWD/jobs/job-dir
        type: Directory
    - name: spark-application-file
      hostPath:
        path: /Users/joaocerqueira/Documents/github/spark-on-kubernetes/jobs/job-dir/dataminer-categorized-delta-analytics.py  # $PWD/jobs/job-dir/dataminer-categorized-delta-analytics.py
        type: File
#
#
  mainApplicationFile: local:///job-dir/dataminer-categorized-delta-analytics.py
#
  arguments:
     - " "
  packages:
     - "io.delta:delta-core_2.11:0.6.1"
#
  sparkVersion: "2.4.5"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 2.4.5
    serviceAccount: spark
#
    volumeMounts:
      - name: local-jobs
        mountPath: //job-dir
      - name: spark-application-file
        mountPath: //job-dir/dataminer-categorized-delta-analytics.py
#
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 2.4.5
#
    volumeMounts:
      - name: local-jobs
        mountPath: //job-dir
      - name: spark-application-file
        mountPath: //job-dir/dataminer-categorized-delta-analytics.py
#
#
