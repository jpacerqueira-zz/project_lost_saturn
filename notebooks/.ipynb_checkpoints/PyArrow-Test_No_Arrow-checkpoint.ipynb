{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /home/notebookuser/anaconda3/lib/python3.7/site-packages (1.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/notebookuser/anaconda3/lib/python3.7/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.14 in /home/notebookuser/anaconda3/lib/python3.7/site-packages (from pyarrow) (1.16.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 468 ms, total: 29.6 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "##############################Execution##########################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "from pyspark import SparkFiles as spf\n",
    "#\n",
    "import subprocess\n",
    "#### subprocess.run('unset PYSPARK_SUBMIT_ARGS', shell=True)\n",
    "subprocess.run('export SPARK_LOCAL_IP=localhost ', shell=True)\n",
    "#\n",
    "### DELTA Runtime start script \n",
    "##### subprocess.run('export PACKAGES=\"io.delta:delta-core_2.11:0.3.0\" ', shell=True)\n",
    "##### subprocess.run('export PYSPARK_SUBMIT_ARGS=\"--packages ${PACKAGES} ${PYSPARK_SUBMIT_ARGS}\" ', shell=True)\n",
    "####\n",
    "####\n",
    "#sc = pyspark.SparkConf().set('spark.sql.execution.arrow.enabled','true').setAppName(\"PyArrow_Test\").getAll()\n",
    "sc = pyspark.SparkContext(appName=\"PyArrow_Test\") \n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "# Creating two different pandas DataFrame with same data\n",
    "pdf1 = pd.DataFrame(np.random.rand(1000000, 3))\n",
    "pdf2 = pd.DataFrame(np.random.rand(1000000, 3))\n",
    "# Letâ€™s test the conversion of Pandas DataFrames to Spark DataFrames first without modifying anything and then allowing PyArrow.\n",
    "%time df1 = sqlContext.createDataFrame(pdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
